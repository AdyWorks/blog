# Differential privacy and correlated data

A recent [blog post](https://freedom-to-tinker.com/blog/pmittal/differential-privacy-is-vulnerable-to-correlated-data-introducing-dependent-differential-privacy/) announces: 

> Differential Privacy is Vulnerable to Correlated Data

This paper continues a line of thought initiated in the [No Free Lunch](http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf) paper [we discussed just a post ago](). Unlike the previous works, this paper is brave enough to make falsifiable statements (those that could in principle be disproved) about differential privacy. 

You can probably imagine what this post will be about.

We will talk through the arguments of the paper and see where they fall down, both in terms of making accurate statements about differential privacy, and rhetorically in characterizing the paper's contribution. As always, there is something to learn from papers like these, and I'll try and extract what it actually shows about privacy, if not differential privacy.

## On words, which have meanings 

Our subject today is a paper, whose abstract starts (emphasis mine):

> Differential privacy (DP) is a widely accepted mathematical framework for protecting data privacy. Simply stated, it guarantees that the distribution of query results changes only slightly due to the modification of any one tuple in the database. This allows protection, even against powerful adversaries, who know the entire database except one tuple. **For providing this guarantee, differential privacy mechanisms assume independence of tuples in the database** - a vulnerable assumption that can lead to degradation in expected privacy levels especially when applied to real-world datasets that manifest natural dependence owing to various social, behavioral, and genetic relationships between users.

The bolded text is false. It is in no sense true. No differentially private mechanism assumes anything about the distribution of their input data, precisely because differential privacy does not permit such assumptions. The guarantee of differential privacy, "that the distribution of query results changes only slightly due to the modification of any one tuple in the database", must hold for all input datasets and single tuple modifications.

The authors continue to write, later

> To provide its guarantees, DP mechanisms assume that the data tuples (or records) in the database, each from a different user, are all independent.

and later still

> However, the privacy guarantees provided by the existing DP mechanisms are valid only under the assumption that the data tuples forming the database are pairwise independent

Each of these statements are false. None of them are supported in the paper, or in any of their citations. The authors should perhaps be slightly embarassed for making false statements in the scientific record, but not as much as NDSS should be embarassed that their reviewers can't distinguish true from false. At the same time NDSS is apparently a conference with such attention to detail that their "programme" announced the talk thusly:

![Programme](https://github.com/frankmcsherry/blog/blob/master/assets/vulnberable/programme.png)

Yes, "vulnberable". Also, while the actual abstract is just one monolithic paragraph and could use a line break or two, there were perhaps better places than mid-sentence.

## An attack on differential privacy

Perhaps we can learn more about the authors' intentions (or prove my sass unfounded) by looking into the details of their attack.

> In Section IV, we perform a real-world inference attack to demonstrate that a DDP-adversary can extract more private information than guaranteed by DP.

Ok, that sounds interesting. A DDP-adversary is one with access to all the data except yours, and some side information about pair-wise relationships between people (in the form of a social network, in their case). Once we get to Section IV, we read:

> In this section, we demonstrate (1) a real-world inference attack on the LPM- based differential privacy mechanism, as a realistic confirmation of the feasibility of such attacks in practical scenarios; and (2) the capability of a DDP-adversary to use released data, satisfying DP definition, to build an inference attack which violates the security guarantees of DP mechanisms.

Here LPM is "Laplace Perturbation Mechanism", where you add some Laplace noise to low-sensitivity queries. I'm already a little worried that the released data both "satisfies the DP definition" and something then "violates the security guarantees of DP mechanisms". The definition and guarantees are ... the same thing.

Let's see what happens next! 

> We consider a scenario where the data provider uses the classical K-means clustering approach [20] on the Gowalla location dataset to compute cluster centroids, applies DP mechanism on the centroids, and publishes the perturbed centroids to other applications or researchers.

Yeah, this isn't differentially private. Sorry. Pre-clustering the data and then perturbing the results does not have the property of differential privacy. In the same vein, reporting the average of 1,000 copies of my data, but adding noise, doesn't satisfy differential privacy. There are k-means clustering analogues which provide differential privacy, but this is not one of them.

Let's give the authors the benefit of the doubt and imagine they got their results using actual differential privacy. The authors next look at the conditional probability that `your data` has some value, given access to the reported `centroids` and `all other user data`. 

	Pr[ your data | centroids + all other user data ]

This is what is going to let them figure out what `your data` is with high confidence! As a quick aside, this term is within a factor of exp(epsilon) of 

	Pr[ your data | all other user data ]

because the distribution of `centroids` doesn't change by more than that factor when you add or remove one record (we are imagining that it is differentially private). The `centroids` measurement doesn't add much more information than is already revealed by `all other user data`.

Screw you differential privacy; you disclose things, and you don't contribute information!

The authors then consider two types of attacks: the data are independent, in which case `your data` has the same distribution as the other data, and the data are correlated through some side information, like a social network that identifies connected records. In the case that the data are correlated, they can learn the value accurately! Just by looking at all the data that you are correlated with. Not really by looking at the centroids or anything; lol forgot about them.

At the moment this has nothing to do with differential privacy, because as mentioned you could just throw away the differentially private measurements and it wouldn't much change your posterior distributions. However, the authors do introduce a theorem relating differential privacy and "leaked information", the change in entropy due to showing someone all the other records (and the `centroids`).

![Leaked](https://github.com/frankmcsherry/blog/blob/master/assets/vulnberable/leaked.png)

![Theorem](https://github.com/frankmcsherry/blog/blob/master/assets/vulnberable/theorem.png)

The authors do observe a substantial change in entropy due to revealing all other records, which makes sense if people hang out with their friends, and that plus this theorem would mean that differential privacy isn't doing its job.

The theorem is incorrect. It is most easily seen by imagining what happens if the centroids had zero-differential privacy. They would contribute zero information, about anything, and so "leaked information" would simplify to  

	Leaked Information = avg_i ( H(Di) - H(Di | all except Di ) )

The theorem says this should be zero. When there are correlations between `Di` and the other records it will not be.

Here is the "detailed proof" from the appendix:

![proof](https://github.com/frankmcsherry/blog/blob/master/assets/vulnberable/proof.png)

You can be excused for not understanding this. What I *think* the authors are saying is that because differential privacy relates the probabilities of adjacent datasets `D` and `D'`, by a factor of exp(epsilon), then the probabilities for any `D` are pretty close to whatever distribution they have in mind (which is what the `.` means, I think). The relation of differential privacy only holds for adjacent datasets, though, not `D` and arbitrary datasets, which is what you would need to perform their integration. I think this is  what goes wrong in their proof. It is admittedly hard to tell.

Authors, NDSS reviewers? Would either of you like to clarify?

For another counter-example to their theorem, we just need to look at their algorithm. Imagining that their algorithm is differentially private, it shouldn't be able to leak more than epsilon information. But it does. 

Proof by contradiction can cut both ways.

### Open question: What about k-means?

What differential privacy does guarantee is that given access to all records except one, the differentially private k-means measurements contain basically no new information. You could generate a stastistically indistinguishable distribution just using all those records you have, without the one held out. As the authors study an attacker who has access to all records except one, what role did the k-means centroids play in the inference? Is the attacker better able to attack with these means than without? That would be what you want to show to incriminate differential privacy.

This isn't evaluated, and my belief (or "theorem") is that the whole argument goes through without the k-means at all. If I have all of your friends locations, and your social links to them, I can guess where you will be. You can do k-means too, but I'm not even going to look at your results.

But, if you get rid of the k-means, differential privacy isn't even party to the argument any more. What the crap. What the hell is this argument about anyhow?

### Accidental insight

The authors conclude with what is probably the most insightful bit in the paper:

> From our analysis, we can see that a differential privacy technique performed on a dependent data set will disclose more information than expected, and this is a serious privacy violation which hinders its applications to real-world data that may be inherently dependent.

The problem absolutely lies in what was *expected*. I can't actually tell what the authors expected, perhaps bounded information leakage, but it clearly wasn't what differential privacy provides. Differential privacy's application is hindered, but by privacy expectations rather than serious privacy violations.

I think it is really important for people, both authors and subjects, to work on sorting out their *privacy expectations*. If you publish your social network and your friends' locations, do you *expect* that your location will remain a secret? You might. Do you expect that publishing differentially private k-means centroids in addition will somehow fix your privacy problems? If that is really the hold-up, then what we need to fix is your expectations.

## A possible explanation

The authors have some point they would like to make and have just, in my opinion, made it quite badly. Here is a re-written version of the abstract which isn't fundamentally inaccurate, and gets closer to the actual point of their paper:

> Differential privacy (DP) is a widely accepted mathematical framework for protecting data privacy. Simply stated, it guarantees that the distribution of query results changes only slightly due to the modification of any one tuple in the database. This allows protection, even against powerful adversaries, who know the entire database except one tuple. **However, differential privacy's guarantees only mask those records *from* each user, they do not mask larger stastistical trends that may reveal information *about* each user.** This distinction can lead to degradation in expected privacy levels especially when applied to real-world datasets that manifest natural dependence owing to various social, behavioral, and genetic relationships between users.

This gets at a *really important point*: differential privacy does not guarantee that secrets about you will remain secrets. This may make differential privacy a non-answer when you need to maintain secrets about people beyond the secret of what data they contributed. What the authors study in their paper (the above is just the first half) are algorithms that suppress statistical trends in the data. That is fine to study, but maybe let's think about whether it is still called "privacy".

All of these examples of correlation: social, behavior, genetic and more, are great examples of where we need to carefully evaluate our privacy expectations. We want our genomic data to be a total secret, but 98% of it sure isn't. Oh, right. Well, we want "the rest" to be secret. How much is still a secret, and for how long?

These are really hard questions to answer, because specific answers actually rely on understanding real dependences in genomic data, and we aren't there yet scientifically. 

On the other hand, we may not need to answer these hard questions to provide actionable privacy guarantees: your contribution of genomic data, when guarded with differential privacy, will reveal no *further* secrets about you. We can't tell you what secrets you started with, or how long you'll have those secrets, but we can guarantee that you aren't disclosing any new secrets by participating.