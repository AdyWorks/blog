# Lunchtime for Data Privacy

This post will be a bit of a philosophical one, about what privacy is and why we might choose to call things that. The story comes in the context of a paper [No Free Lunch in Data Privacy](http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf) which argues that one can't have everything when it comes to statistical analysis of private data: if you want some accuracy you have to give up some privacy.

I think they are wrong, in that their definition of privacy fails to distinguish between "your secrets" and "secrets about you". 

The distinction between "your secrets" and "secrets about you" is important, not least because as a society we place great value on one's right to self-determination, and somewhat less value on one's right to image management. If we must trade away some secrets for accurate statistics in the common interest, it is helpful to distinguish the types of secrets we might be trading away, and it may be harmful to fail to distinguish them.

To give an example, imagine you reveal to the world that you smoke, perhaps by smoking publicly, and independently the world learns through some health survey that smoking leads to increased health risks. From these two pieces of information we learn a little about you and your health risks. Prior to the survey's results your health risks may have been *a* secret, in the sense that they were not yet widely known, but they stopped being *your* secret when you made public the information that you smoke. I would argue that releasing the results of the health survey are not a data privacy violation (as you may not even have participated).

While this may sound a bit fluffy, differential privacy formally distinguishes between "your secrets" and "secrets about you". Implementations of differential privacy demonstrate that you do *not* need to trade away any of "your secrets" to get some accurate statistical information. On the other hand, it does not guarantee that "secrets about you" remain (or ever were) secret, for a good reason we will get to.

If you fancy this definition of data privacy for lunch (perhaps yours was eaten), it is on the house.

## No Free Lunch

The No Free Lunch in Data Privacy paper is organized around a "[no free lunch](https://en.wikipedia.org/wiki/No_free_lunch_theorem)" type of theorem, one which says that for a technique to do one thing well, it must do other things badly. This makes some sense in the context of data privacy, where there is a tension between the two main goals, the privacy of individuals and accuracy across large populations. If you want to do one very well, you may need to do the other badly. 

The theorem says that if you have a secret about a dataset---the authors give the example of reading the first record in the dataset---then any non-useless computation can take the secret from "totally secret" to "pretty much public". This would suck; records in the dataset feel very much like a "your secret" type of secret. 

Fortunately (for us, not the authors), the proof is wrong and the theorem is false. 

To understand, let's look at the definition of non-useless the authors use, which they call *discrimination*.

![Discriminant](https://github.com/frankmcsherry/blog/blob/master/assets/nolunchforyou/discriminant.png)

Informally, discrimination says that there should be at least a few input datasets that are likely to produce different sorts of results. This would mean that if you see one of the results, it probably wasn't one of the other inputs (with parameters set appropriately).

The theorem uses this intuition to reverse out the precise value of a secret, from any discriminating computation:

![NoLunch](https://github.com/frankmcsherry/blog/blob/master/assets/nolunchforyou/nolunch.png)

The proof picks datasets with different values of the secret as inputs, and asks the computation to discriminate between them, at which point it knows the input dataset and the secret. Unfortunately, the definition of discrimination doesn't require a computation to distinguish between *any* datasets, just *some* datasets. Maybe not *these* datasets.

As a counter-example to the theorem, consider a query which wants to read out my record, and a computation which says "buzz off" if my record is present in the input (but is discriminating on inputs without my record). There are several values my record might take, but for any dataset where one is present the computation acts identically ("buzz off"). The attacker can learn if my record is present or absent (a non-"buzz off" answer), but learns nothing about its value.

## No Free Breakfast

The No Free Lunch theorem is meant to be a simplification of [prior work by Cynthia Dwork and Moni Naor](https://www.microsoft.com/en-us/research/publication/differential-privacy/), in which they prove that any non-useless computation can reveal a secret to an appropriately informed attacker. While the No Free Lunch result may not be correct, does the prior more complex work nonetheless mean lunch cannot be cheaply had?

The No Free Lunch authors present their theorem as a simplified and consequently weaker version of Cynthia and Moni's, but it is actually a strictly stronger statement. Specifically, their result is false where as Cynthia and Moni's is not. That is a fairly unhelpful way to understand *how* Cynthia and Moni's result is weaker than the No Free Lunch claim. 

To understand what Cynthia and Moni proved, let me try and paraphrase their result (not actually a quote).

> Imagine a computation that tells us something "non-obvious" about a dataset, in the sense that we don't already know the answer for sure. More specifically, imagine an attacker who doesn't know the answer for sure. What the attacker *does* know is that if the answer goes one way the secret is `A`, and if the answer goes the other way the secret is `B`. If the computation is run and an answer produced, the attacker is now certain of the secret! Oh noes!

I've used "the secret" rather than "your secret", because the proof only works if your secret is first revealed to a third party, who can give a hint to the attacker. If you retain the ability to change your secret, perhaps because you haven't yet submitted your data, then the attacker still reaches the same conclusions with the same certainty, despite now being totally wrong. 

The hypothetical attack only works against "secrets about you" rather than "your secrets".

If this all sounds like a bit of a mess, great! It was meant to demonstrate that so called "absolute" privacy definitions, those of the form "no one will learn secrets about you", are mostly hopeless. Anything non-useless can reveal a "secret about you", unless you wish to make restrictive assumptions about the attack and attacker.

At the same time, the stronger No Free Lunch statement, that anything non-useless can be made to reveal "your secret", is false.

The distinction between these two types of secrets, those you control and those you do not control, is exactly what lead us to define differential privacy as we did so many years back. 

## Anyhow ...

The No Free Lunch paper continues to "address several popular misconceptions about differential privacy", that several "assumption-free" aspects of differential privacy are actually in error.

![Misconceptions](https://github.com/frankmcsherry/blog/blob/master/assets/nolunchforyou/misconceptions.png)

To be clear, each of these statements are correct as written. Differential privacy protects "your secrets" without assumptions on data generation or attacker knowledge. The authors' treatment of them as misconceptions is predicated on at least one of (i) their theorem being true or (ii) data privacy meaning "secrets about you" rather than "your secrets". As we've read (i) isn't, and assumptions are needed for (ii) for any non-useless computation, differentially private or not.

The authors do provide several examples where you might have felt entitled to "secrets about you" that are no longer "your secrets", for example when an authority compels the release of exact partial information from you (e.g. "you must smoke outdoors"). This seems like a healthy subject for research, but to the extent that an information disclosure can result without access to (or even the existence of) your data, I would not call it a privacy breach.

Obviously it is fun to call things "privacy", because you get a lot more traction when you do, but it might help readers to think harder about why that term is appropriate, and for authors to argue more clearly that it is. It would be especially helpful to be clear when you claim that techniques do not provide privacy, that you mean a form of privacy that has been proven impossible without further assumptions.