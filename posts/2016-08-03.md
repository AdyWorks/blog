# Differential dataflow internals; a work in progress

This post is really meant to be a series of posts about how to represent data managed in [differential dataflow](https://github.com/frankmcsherry/differential-dataflow). As they are topically related and meant to build upon one another, I thought putting them in one place would help record and explain what I was thinking when implementing them. They *may* also be helpful for folks thinking about building interesting things in Rust!

In the spirit of "append only updates", this post will largely be append-only (modulo errors, edits). I'm also going to try and make the development process largely append-only as well: new implementations will be added to older ones, rather than replacing them, allowing us to keep track of where we were and how far we've come. This will probably break at various points, but that is the intent!

This series is going to start out intentionally pedantic and simple, to frame the problem in the simplest language and implementations. We will try and become smarter as we move along, but let's start with simple, and evaluate things as we go. Here is the planned roadmap:

* [Part 0/x: Trait definitions; defining what we require.](https://github.com/frankmcsherry/blog/blob/master/posts/2016-08-03.md#part-0-trait-definitions-defining-what-we-require)

* [Part 1/x: An obvious implementation based on `HashMap`.](https://github.com/frankmcsherry/blog/blob/master/posts/2016-08-03.md#part-1-an-obvious-implementation-based-on-hashmap)

* [Part 2/x: A very na√Øve implementation based on sorted lists.](https://github.com/frankmcsherry/blog/blob/master/posts/2016-08-03.md#part-2-a-very-naive-implementation-based-on-sorted-lists)

* Part 3/x: Organizing the data into tries.

* Part 4/x: Adding an index.

* Part 5/x: ...

I'd like to think I know exactly where this is going to end up (I have a plan), but for now let's just pretend I know the general direction, and will tell you each time we take a meaningful step.

## Part 0: Trait definitions; defining what we require.

One of the things I like doing most when writing Rust code is to write trait definitions. These are the "interfaces" of Rust, which indicate the methods and types and such required in order to actually implement "a thing". For example, we are going to spend some time defining and implementing collections, and we need to specify what methods and such these collections need to provide to be generally useful.

One nice aspect of Rust is that just writing trait definitions gives you some insight into how your implementations are going to need to work. For example, all methods in Rust clearly indicate the *ownership* of input arguments and returned output. Ownership distinguishes between variable bindings that *own* the data, and can do pretty much whatever with it (including de-allocating it), and variable bindings that only *reference* the data (the sigil `&` indicates references at work). Just writing down the method signatures gives some clarity about which types own which data, who gets to refer to them, and for how long they will be around. Each of these decisions constrains your implementation, and even at this point it helps you think out which details implementations should commit to, and which they should be able to hide.

### An example trait definition

Let's try out a trait definition for the "collection trace" functionality, something that differential dataflow uses. In order to not make you angry with details I'm not going to start with the final version. I hope this ends up clearer than otherwise, and I suspect once you see the result you may agree.

Our collection trace tracks tuples `(key, val, time, delta)` of type `(Key, Val, Time, isize)` where the types `Key`, `Val`, and `Time` get to be chosen by the person who is instantiating our type. These generic parameters are indicated in the name of the trait, written as part of its definition as:

```rust
pub trait Trace<Key, Val, Time> {

	// methods and stuff goes here

}
```

In fact our trace is not going to work for any arbitrary types; there are constraints that the types need to satisfy. For example, if we want to find keys we had best be able to test keys for equality, right? Equality testing is defined by a trait, `Eq`. We can add the constraint that whichever type `Key` gets chosen implements equality testing by adding a constraint:

```rust
pub trait Trace<Key: Eq, Val, Time> {

	// methods and stuff goes here

}
```

See how we added `: Eq` after `Key`? That means that the choice of `Key` needs to also implement the trait `Eq`, which is great to know because from this point forward we can rely on it being true, and write things like `if key1 == key2` without knowing yet which types of key we will need to use.

Our trace actually has different constraints; the keys and values are going to be *ordered*, and the times must form [a lattice](https://github.com/frankmcsherry/differential-dataflow/blob/82783d89405ed28658c6c25c98d30750dd05c2ac/src/lattice.rs#L3-L28), which is like a not-exactly-ordered set (it is a partially ordered set with some additional structure).

```rust
pub trait Trace<Key: Ord, Val: Ord, Time: Lattice> {

	// methods and stuff goes here

}
```

Even just writing this much is helpful, because we have sorted out (or started too, at least) what needs to be true about the types we will use.

### How about adding some methods? 

Yes, let's add some methods. There are two things most collections do: add data and get data. I'm going to write some over-simple versions of these that would be great, but lack some flexibility. Our data happens to be tuples `(key,val,time,delta)`, so we are going to add and retrieve vectors of those.


```rust
pub trait Trace<Key: Ord, Val: Ord, Time: Lattice> {

	/// incorporates a new bunch of differences into the trace.
	fn add(&mut self, diffs: Vec<(Key, Val, Time, isize))>);

	/// retrieves differences associated with a specified key.
	fn get(&mut self, key: &Key) -> Vec<(Key, Val, Time, isize)>;

}
```

You may notice the `&` sigil hanging around in a few places. We will get to that, but what it is saying is that when we call `add` or `get`, we are only temporarily working with `self`, the trace. Once the method returns, control of `self` returns too. Similarly, the use of `key` is only temporary access to the key. 

This contrasts with `diffs` in `add`: there is no `&` in front of the type, which means that `diffs` is actually an *owned* vector; we get to do whatever we want with it (add things to it, de-allocate it, etc). This ownership is pretty handy, because it is transitive: `diffs` also owns all of the contents of the vector, meaning all those keys and values and times and stuff. We will want to put those in our collection!

Looks pretty good! What's not to like? Well, there are a few things. 

### Type parameters: letting the user choose

Adding and returning vectors is not unreasonable, but putting that detail in the trait definition limits our implementations. Does `add` really need the results to be materialized in a vector, and does `get` really need to allocate and return ownership of memory?

Not really. I mean, they could and that might be useful, but we could be more general by indicating that all `add` really needs is a way to enumerate the items you would like to add, and all `get` really needs to return is a way for you to enumerate the results. These could be backed by vectors, but they don't have to be. There will be better choices, it will turn out.

We have already seen the techniques that we will use to improve the `add` method: generic type parameters. The method can introduce a new generic type parameter, `Iter`, whose only requirement is that it implements the `Iterator` trait with appropriately typed items, and then accepts `diffs` as an `Iter` rather than a vector:

```rust
pub trait Trace<Key: Ord, Val: Ord, Time: Lattice> {

	/// incorporates a new bunch of differences into the trace.
	fn add<Iter: Iterator<Item=(Key, Val, Time, isize)>(&mut self, diffs: Iter);

	/// retrieves differences associated with a specified key.
	fn get(&mut self, key: &Key) -> Vec<(Key, Val, Time, isize)>;

}
```

All that we have done here is said that when one implements `add` one needs to be able to respond to arbitrary iterator types, without relying at all on the concrete type (no `Vec`-specific methods, for example). This has the advantage that when we *use* the method, we can use whatever iterators we like, including ones that don't stage the results in one vector before inserting them (e.g., timely dataflow delivers messages as small batches, and we may want to insert the results of concatenating all of these, without copying them to another location in memory first).

Before you get all "lol iterators", these type constraints mean that methods are available statically, and can be compiled down to roughly the sort of for-loops you would write by hand. Some times they are even better (some times they are worse). But, there are real performance differences between an iterator that reads a list of block of data, and an implementation that first copies them to a second contiguous allocation.

### Associated types: letting the implementation decide

What if the `get` method wants to return an iterator, but be coy about which specific iterator type it returns? It doesn't really make sense for the user to ask for an iterator type, because it is the implementation that has to determine what gets returned. Rather, it makes sense for the implementation to define the type of the result.

In Rust traits may define "associated types", which are a bit dual to the generic type parameters that users get to specify. These are types that the implementor of the trait determines, and other than constraints on the types nothing else is exposed through the interface of the trait. We can indicate that an implementor of `Trace` must name a type for the output of `get` and that this type must implement the `Iterator` trait with approriate items, just like so:

```rust
pub trait Trace<Key: Ord, Val: Ord, Time: Lattice> {

	/// incorporates a new bunch of differences into the trace.
	fn add<Iter: Iterator<Item=(Key, Val, Time, isize)>(&mut self, diffs: Iter);

	/// An iterator defined by the implementation to produce results.
	type OutputIterator: Iterator<Item=(Key, Val, Time, isize)>;

	/// retrieves differences associated with a specified key.
	fn get(&mut self, key: &Key) -> Self::OutputIterator;

}
```

Notice how `get` now returns `Self::OutputIterator`, a type totally unknown to us at this point, with only the promise that any implementor of `Trace` will need to be more specific. That's fine, we can be more specific when we implement things, and Rust can stitch everything together then. The flexibility the implementors get to choose different iterator implementations is great, though.

### Returning references: learning about lifetimes

Ok, brace yourselves.

The signatures of the methods have so-far involved *owned* data. When `get` returns an iterator whose items are tuples `(key, val, time, isize)` it yields tuples whose contents are owned by the recipient. That is great for the recipient, who can now do whatever they like with the results, but it can be expensive and restrictive for the implementation.

To make this more concrete, imagine that the key type is `u64` and the value type is `String`. A `String` is dynamically sized, depending on how much stuff you stuck in there. That means that a `String` involves some dynamically allocated memory, which is usually a bit annoying to go and grab more of when you don't need to. But, through our signatures we have promised to return actual *owned* `String` types, meaning the responsibility for the memory involved is passed from the trace implementation to the recipient. Unless the trace is getting rid of its copy of the string (no) it probably needs to make a copy, because you might decide to call `.all_caps()` on it.

If the recipient (you) just needed to *look* at the `String`, you might feel a bit silly for forcing all these allocations just to subsequently de-allocate them.

This is where Rust's references come in. References are not owned data, but rather "references" to data owned by someone else. When you get a reference, you get to look at it, maybe even mutate it, but once you are done with it (and you do have to be done with it, eventually) the owner gets control back. Rust has a bunch of rules in place to make sure that when you borrow a reference no one else mutates the referenced data, and part of the joy of Rust is learning to interpret the various reasons Rust won't let you do the things you wanted to do with references.

Let's investigate this 'joy' (50-50 sarcastic-serious, so single quotes) by trying to return *references* to key, value, and time data, rather than owned instances of these types.

```rust
pub trait Trace<Key: Ord, Val: Ord, Time: Lattice> {

	/// incorporates a new bunch of differences into the trace.
	fn add<Iter: Iterator<Item=(Key, Val, Time, isize)>>(&mut self, diffs: Iter);

	/// An iterator defined by the implementation to produce results.
	type OutputIterator: Iterator<Item=(&Key, &Val, &Time, isize)>;

	/// retrieves differences associated with a specified key.
	fn get(&mut self, key: &Key) -> Self::OutputIterator;

}
```

We just put `&` in front of the results in the `OutputIterator` contraint. Aside: I didn't put a reference in front of the `isize` because we know we can always just copy that data.

This doesn't work, and Rust tries to help us out by saying:

	src/lib.rs:44:45: 44:49 error: missing lifetime specifier [E0106]
	src/lib.rs:44     type OutputIterator: Iterator<Item=(&Key, &Val, &Time, isize)>;
                                                          ^~~~
	src/lib.rs:44:45: 44:49 help: run `rustc --explain E0106` to see a detailed explanation

What is a lifetime specifier? You could totally run `rustc --explain E0106` to find out, and I was going to copy/paste it here but it is really quite long. Instead, I will try and explain.

Rust needs to be able to determine that references to the same data are not active at the same time, because that is what guarantees that no one is messing with your data while you are looking at it (or looking at your data while you mess with it). This is done through the concept of "lifetimes", an indication of "for how long" the reference is in play. Here "how long" is measured in "parts of your code" rather than something like real time.

For each reference you take, Rust infers the span of code for which the reference is live, and bakes that into the type. The problem here is that we haven't told Rust enough for it to figure out the lifetime for these references. Let's write something slightly less clever, but which shows where the lifetimes would come from; we will temporarily return to the more innocent time where we returned a `Vec` output rather than an iterator:

```rust
	/// retrieves differences associated with a specified key.
	fn get(&mut self, key: &Key) -> Vec<(&Key, &Val, &Time, isize)>;
```

This almost works. If it weren't for that `key` argument, Rust *would* be able to figure out the lifetimes of our outputs. 

Why? Because the only things the results could possibly refer *to* are the references supplied as inputs (or references available through them, transitively). What else could these reference possibly refer to that will still be valid after the method returns?

What is going on here, or would be going on without the `key` argument, is what Rust calls "lifetime elision", some handy rules Rust uses to remove the need to explicitly write lifetimes everywhere. But you can write them explicitly, and it is helpful to see to understand what Rust is actually doing. Let's fix that method up above with the right generic lifetimes:

```rust
	/// retrieves differences associated with a specified key.
	fn get<'a, 'b>(&'a mut self, key: &'b Key) -> Vec<(&'a Key, &'a Val, &'a Time, isize)>;
```

Whoooooaaaaa! All of the `&` things have some more noise around them! 

The most important thing to notice above is that `get` has two generic parameters, `'a` and `'b`, which are lifetime parameters. Their roles are to capture the lifetime information about the references supplied by the caller, and to say that in the vector of results, all those references have the same lifetime as the `self` reference rather than the `key` reference.

This makes a lot of sense, right? The references are totally into the trace, rather than at the query key. It's cool, all we need to do is tell Rust that. In exchange, Rust will make sure no one accidentally puts `key` into the result vector, as its referrent might get mutated or invalidated as we spin around a loop. 

You wouldn't do that, of course. I trust you, but Rust doesn't.

So, we need to put some `'a` things in front of our references to make Rust happy. For "reasons", we need to add it as a generic parameter for the trait, rather than the method. What reasons? Because the `OutputIterator` type constraint depends on `'a`, not just the `get` method, and Rust doesn't yet allow associated types to have generic parameters (part of "higher kinded types" or something).

```rust
pub trait Trace<'a, Key: Ord, Val: Ord, Time: Lattice> {

	/// incorporates a new bunch of differences into the trace.
	fn add<Iter: Iterator<Item=(Key, Val, Time, isize)>>(&mut self, diffs: Iter);

	/// An iterator defined by the implementation to produce results.
	type OutputIterator: Iterator<Item=(&'a Key, &'a Val, &'a Time, isize)>;

	/// retrieves differences associated with a specified key.
	fn get(&'a mut self, key: &Key) -> Self::OutputIterator;

}
```

You might be a bit stressed that "how can we known which `'a` we will need" to which the answer is just "no worries, we will implement the trait for all `'a`", which is a great thing that computers can do.

What is the last remaining issue? The types parameters `Key`, `Val`, and `Time` need some more constraints on them now. 

	src/lib.rs:33:1: 66:2 error: the parameter type `Key` may not live long enough [E0309]
	src/lib.rs:33 pub trait Trace<'a, Key: Ord, Val:Ord, Time: Lattice> {
	              ^
	src/lib.rs:33:1: 66:2 help: run `rustc --explain E0309` to see a detailed explanation
	src/lib.rs:33:1: 66:2 help: consider adding an explicit lifetime bound `Key: 'a`...
	src/lib.rs:33:1: 66:2 note: ...so that the type `Key` will meet its required lifetime bounds

What Rust is telling us now is that because we are thinking about returning types like `&'a Key` we have to promise that `Key` itself is actually valid for all of the lifetime `'a`. We could make `Key` something horrible like `&'b String` for some other `'b`, at which point the reference `&'a Key` wouldn't actually be to valid data. You probably weren't thinking you would do this, but Rust was. Rust is protecting you from weird creative you, and from your weird creative co-workers, and the weird creative unintended interactions of your code with theirs.

The fix is pretty simple:

```rust
pub trait Trace<'a, Key: Ord+'a, Val: Ord+'a, Time: Lattice+'a> {

	/// incorporates a new bunch of differences into the trace.
	fn add<Iter: Iterator<Item=(Key, Val, Time, isize)>>(&mut self, diffs: Iter);

	/// An iterator defined by the implementation to produce results.
	type OutputIterator: Iterator<Item=(&'a Key, &'a Val, &'a Time, isize)>;

	/// retrieves differences associated with a specified key.
	fn get(&'a mut self, key: &Key) -> Self::OutputIterator;

}
```

The constraint `Key: Ord+'a` means that `Key` needs to implement `Ord` and live at least as long as `'a`, which mostly just means it can't contain references that might last less long. Typically `Key` is going to be some owned data like `u64` or `String` or even `Vec<Vec<Option<String>>>`, all of which satisfy this constraint.

### An observation

This whole section may give some insight into the Rust debugging process. We have done a lot of up-front work defining our trait, and scoping out several potential issues that Rust has already forced us to fix by making our method prototypes more specific. I would say that the majority of my time debugging Rust code is spent like this. I've literally spent zero time in an actual debugger, and once written while not necessarily correct, your code pretty much does what it says.

## Part 1: An obvious implementation based on `HashMap`

Just to start things out, and before trying to be clever at all, let's write an implementation of our trait using Rust's built in `HashMap`. This implementation will be pretty simple, but wasteful with respect to memory usage, and misses out on some important properties we will want for later on. But, it is a simple example of perhaps the most obvious implementation.

This also gives us a chance to see how to implement traits in Rust, before we start doing harder things.

To implement a trait for a type, here a hashmap from keys of type `K` to vectors of `(V, T, isize)` elements, you write something like so:

```rust
impl<'a, K, V, T> Trace<'a, K, V, T> for HashMap<K, Vec<(V, T, isize)>> {
	
	// implementation of methods and stuff goes here

}
```

We just need to add constraints on the type parameters and implement the methods, and we should be good to go. 

### Implementing the methods

We have two methods to implement, `add` and `get`. We know what they are going to look like because of the trait's signature, so let's write down some empty bodies.

```rust
impl<'a, K, V, T> Trace<'a, K, V, T> for HashMap<K, Vec<(V, T, isize)>> {

	fn add<Iter: Iterator<Item=(K, V, T, isize)>>(&mut self, diffs: Iter) {
		unimplemented!()
	}

	fn get(&'a mut self, key: &K) -> Self::OutputIterator {
		unimplemented!()
	}

}
```

These `unimplemented!()` things are macros indicating the code isn't written yet. They are totally optional, but allow your code to compile without actually returning the right thing yet (`Self::OutputIterator`, which is also not written yet).

#### The `add` method

Here is what I would do for the `add` method: 

```rust
	fn add<Iter: Iterator<Item=(K, V, T, isize)>>(&mut self, diffs: Iter) {
		for (key, val, time, delta) in diffs {
			self.entry(key)
				.or_insert(Vec::new())
				.push((val, time, delta));
		}
	}
```

There is some syntactic sugar in the form of the `for .. in ..` construct for iterators; because `Iter` implements `Iterator`, we can just use the for loop.

We've also used `HashMap`'s functionality to pretty ergonomically try to find the entry associated with a key, create one if it doesn't exist, and the push the value, time, and delta triple to the end of the list.

This was pretty painless, but we got lucky in a few ways. The `entry` method requires an *owned* instance of `K` because if the entry doesn't exist we need to add it, and the hash map will need to hold on to a copy of the key in that case. However there is only one owned instance of the key for each element in `diffs`, so we were pretty smart to only stash the `(val, time, delta)` triple.

If we tried to use the key twice we would get an error from Rust indicating that we are trying to re-use a variable which has been moved somewhere else. We could consider copying or cloning the key, but that functionality lies behind traits we haven't required of the type `K` yet.

#### The `get` method

The `get` method is going to turn into a pain in the butt, mostly because of the restrictive way Rust's `HashMap` is implemented. We will fight against it a little bit, explain how it could be done, and then walk away because we don't really want to be using `HashMap` anyhow.

Here is what we would like to write:

```rust
	fn get(&'a mut self, key: &K) -> Self::OutputIterator {
		Self::OutputIterator::new(self.get(key))
	}
```

This looks up `key` in `self` and returns an `Option<&'a Vec<(V, T, isize)>>`. We can use that reference in our output iterator! In particular, a `&'a Vec<(V, T, isize)>` gives us access to all the `&'a V` and `&'a T` references contained therein.

The problem is we don't have a `&'a K` anywhere. That `key` argument is a reference, but not with the right lifetime (remember the cautionary tale from the previous section?). The key obviously exists in the `HashMap`, but we can't get a reference to it because `HashMap`'s interface is a bit restrictive. There is a way to get a reference to a key once you've used the `entry` method up above, but that requires an owned key to call in the first place. We can't even stash a second copy of the key in the list with the `(V, T, isize)` tuples because we can't copy or clone it.

Sucks. 

We could consider changing the iterator in the trait definition to be 

```rust
	Iter: Iterator<Item=(&'a V, &'a T, isize)>
```

because we know that users must have a `&K` on hand to call the `get` method. That isn't a horrible idea, but it is limiting; the user has to make sure to match up their keys with the iterators' values, even though we have a reference to the right key in the hash map.

Since we aren't acutally going to use this version of the code, let's imagine that the `HashMap`'s `get` method return a reference to both the key and the value, rather than just the value. We still get to write:

```rust
	fn get(&'a mut self, key: &K) -> Self::OutputIterator {
		Self::OutputIterator::new(self.get(key))
	}
```

because we are just passing the buck to the `OutputIterator` type's `new` method, which we will see next.

### Implementing the types

The `OutputIterator` type isn't going to be wildly complicated. In fact, we could possibly borrow some existing iterator implementations, but we will just roll our own to see how it works. 

The first thing to do is to define the struct that holds the data we will need for iteration.

```rust
/// iterates over tuples of references
pub struct OutputIter<'a, K:'a, V:'a, T:'a> {
	data: Option<(&'a K, &'a Vec<(V, T, isize)>, usize)>
}
```

The `data` field here is, optionally, a triple of: key reference, vector reference, and a `usize` for remembering where we are in the vector. As we advance through the iterator we will increment this `usize`, and stop when it reaches the length of the vector. 

The field is an `Option` type, which can either be `Some(x)` for some data `x`, or `None` indicating that there is nothing there. The option is important here because `get` does need to return a valid instance of `OutputIter` even when the key is absent and no `&'a K` even exists. We could change our `get` trait to return an `Option<Self::OutputIterator>`, or we can just make our iterator return nothing.

Let's first write the `new` method we know we need:

```rust
impl<'a, K:'a, V:'a, T:'a> OutputIter<'a, K, V, T> {
	pub fn new(input: Option<(&'a K, &'a Vec<(V, T, isize)>)>) -> Self {
		OutputIter {
			data: input.map(|(key, vec)| (key, vec, 0))
		}
	}
}
```

Here we see for the first time struct field initialization; the `data` field of an `OutputIter`.
We take in the type produced by `HashMap`'s hypothetical `get` method, and use `map` which applies some logic to the internals of an `Option` type, where in this case we've just put a zero in for the current position. In case `input` is `None`, we just end up with a `None` output and no logic applied.

All we need to do to make `OutputIter` into an iterator is implement the `Iterator` trait. It's really not that hard; I've written the skeleton here to give you a sense for all that is needed:

```rust
impl<'a, K:'a, V:'a, T:'a> Iterator for OutputIter<'a, K, V, T> {

	type Item = (&'a K, &'a V, &'a T, isize);

	fn next(&mut self) -> Option<Self::Item> {

	}

}
```

An implementation needs to indicate the type of the `Item` it will enumerate, and a function that produces the next item or `None` if the iterator has finished. 

```rust
impl<'a, K:'a, V:'a, T:'a> Iterator for OutputIter<'a, K, V, T> {

	type Item = (&'a K, &'a V, &'a T, isize);

	fn next(&mut self) -> Option<Self::Item> {
		if let Some((ref key, ref vec, ref mut pos)) = self.data {
			if *pos < vec.len() {
				*pos += 1;
				let vtd = &vec[*pos-1];
				Some((key, &vtd.0, &vtd.1, vtd.2))
			}
			else {
				None
			}
		}
		else {
			None
		}
	}
}
```

The `if let` construction is a neat thing Rust borrowed from Swift (perhaps "copied" would be more accurate, or "cloned" depending on your views on whether ideas have owners). The construct fires if the pattern matches, in this case if `self.data` is in fact something. At the same time, it binds `key`, `vec`, and `pos` to references to the fields of the something. In the case of `pos` it is even a mutable reference. The `ref` keyword might be a bit confusing, but I guess the Rust folks thought it was better than `*`, which is logically what you want there (bind `key`, `val`, `pos` to things that derefence to the fields).

This trait implementation is all it takes to be an iterator. 

If you use this type in your code, perhaps by calling `get` on our `HashMap` implementation of `Trace`, Rust will go track down this code and use it directly. For example, imagine your value and time types are stack-allocated, like maybe `u64`s or something, and you write the following code:

```rust
let mut dataz = Vec::new();
for (k,v,t,d) in trace.get(&my_key) {
	dataz.push(v.clone(), t.clone(), d);
}
```

In principle Rust (via LLVM) is able to realize that (i) if the iterator's `Option` is `Some` it stays `Some` and that test can be hoisted out of the loop, (ii) the "increment and test against length" pattern is just walking through an array, and (iii) your key, time, and delta accesses have the same layout as where they are stored. All of these combine to allow LLVM to optimize your loop down to a conditional `memcpy`, just checking if your key is present in the `HashMap` and then `memcpy`ing the results if so.

That's pretty neat. It's the sort of thing you might implement by hand in other languages in order to have awesome performance. Instead, you can spend your time writing blog posts about how you don't necessarily need to do that any more.

### The implementation

In addition to the definition and implementation of `OutputIter`, here is our implementation of `Trace`. 

```rust
impl<'a, K:Ord+'a, V:Ord+'a, T:Lattice+'a> Trace<'a, K, V, T> for HashMap<K, Vec<(V, T, isize)>> {

	fn add<Iter: Iterator<Item=(K, V, T, isize)>>(&mut self, diffs: Iter) {
		for (key, val, time, delta) in diffs {
			self.entry(key)
				.or_insert(Vec::new())
				.push((val, time, delta));
		}
	}

	type OutputIterator = OutputIter<'a, K, V, T>;

	fn get(&'a mut self, key: &K) -> Self::OutputIterator {
		Self::OutputIterator::new(self.get(key))
	}

}
```

One last error: to use `K` as a key in a `HashMap` it must implement `::std::hash::Hash`, the trait for hashing. So, we just `+` that into our constraints on `K` and we are good to go!

Except that `HashMap` doesn't actually have the `get` method we wanted. Oh well.

### Wrap-up

I hope this was somewhat informative about how to *implement* things in Rust. We saw some horrible sticky details in the interface mismatch between `HashMap` and `Trace`, where obviously I like my version more but the Rust folks probably have opinions too. My recollection is that they felt the ergonomics of `get` were important as `HashMap` would be used a lot, and so while expressivity is good too, overwhelming people in such a commonly used class might suck.

In addition to the limited interface, there are other things not to like about a `HashMap` implementation. A `HashMap` uses a lot more memory than we want to use; if a key has just one value, well you can do the math on how much this requires (hint: lots). Also, `HashMap` maintains its keys in something of a weird order; it makes sense when you reverse it out, but it makes it painful to pre-arrange keys to provide sequential access. The `Vec` values also result in lots of random access as locality in the hash map means little for the memory they manage.

## Part 2: A very na√Øve version based on sorted lists.

Let's write a relatively na√Øve implementation of our `Trace` trait using sorted lists.

How is this going to work? Sorted lists aren't a standard data structure for maintaining indexed data sets. If we had a sorted list of all the data we could look things up, using binary search or something similar, but how do we update a list? If we need to keep it in order that can be very expensive, right?

One standard approach, I'm not sure who gets credit for it, to maintain not one but multiple sorted lists of varying sizes. When we add new records we merge them in to the shortest of the lists, and as the short lists get bigger we merge them with the large lists. This has the downside that we need to logically merge the lists when we look at them, and the more concrete downside that the values for a key are not simply a `&[(K,V,T,size)]` contiguous slice. At the same time, with updates that probably wasn't going to happen anyhow.

The strategy we are going to use is to merge any lists whose lengths are within a factor of two. This ensures that we have at most a logarithmic number of such lists and that the total work for each element is also logarithmic.

Here is our noble data structure, a list of lists:

```rust
pub struct Lists<K, V, T> {
	lists: Vec<Vec<(K, V, T, isize)>>,
}
``` 

We will set this up so that the largest lists come first, and smaller lists later on. This makes it easy to push and pop things at the end of the list, which is a bit easier for Rust (whose `Vec` type supports vacancies at the end of its allocation, but not at the beginning).

### A quite na√Øve implementation

We now have some functions to implement, na√Øvely of course, for the `Trace` trait.

#### Adding elements

We can be a bit lazy for now and make adding elements relatively simple. Let's do that, and we will improve the behavior once we start looking at performance (just a bit further down this section). 

```rust
fn add<Iter: Iterator<Item=(K,V,T,isize)>>(&mut self, diff: Iter) {

	// collect iterator contents into a temporary buffer.
	let mut buffer: Vec<(K,V,T,isize)> = diff.collect();

	// while the next smallest list is less than twice as large as the buffer, merge it in.
	while self.lists.len() > 0 && self.lists[self.lists.len()-1].len() < 2 * buffer.len() {
		buffer.extend(self.lists.pop().unwrap().into_iter());
	}

	// sort the buffer and push it on the end of our lists.
	buffer.sort();
	self.lists.push(buffer);
}
```

The main bit of laziness here is that rather than merge sorted lists, which we should be able to do in time linear in the length of the lists, we are just concatenating and sorting the lists. It's worse by a logarithmic factor, asymptotically, but for the moment it is a bit easier to write. Plus if we don't do it now, we can see the difference when we fix the implementation!

Another bit of laziness that we will need to address, eventually, is that of consolidation: if we have two records `(k,v,t,1)` and `(k,v,t,-1)` the two should be accumulated and canceled out. More generally, whatever their `delta` values are, they should be added up and put into a single record. This will be pretty easy with sorted lists, and was set to be a big pain in the bottom with the `HashMap` implementation from before. As it turns out, consolidation can be a nice performance win too: despite doing more work, it would seem, we can often reduce the amount of data we are working with.

#### Getting elements

The retrieval side of things is a bit more complicated, in part because we have a more than one list and in part because we have to define and implement an iterator.

In principle, we don't have to do all that much to define an iterator; for a given key, we should look it up in each list and walk over the elements there. That is fine and dandy, and we will start with that. However, what we *actually* would like is to iterate over the records in sorted order; we would like like to merge the sorted sub-lists, to present the appearance of sorted data. Why? Because we are heading towards a more sophisticated interface where one can *navigate* rather than *iterate* through the resulting tuples. More in a bit.

Let's take the simple approach for now. We look up the key in each list, and if it is present we add its range of tuples to a list of such slices. Importantly, we don't actually iterate through all of the values beforehand; if the user doesn't want to use the iterator, the cost to building it shouldn't be large.

```rust
fn get(&'a mut self, key: &K) -> Self::OutputIterator {
	// storage for slices
	let mut slices = Vec::new();
	for list in &self.lists {
		let lower = binary_search(list, |x| &x.0 < key);
		let upper = binary_search(list, |x| &x.0 <= key);
		if lower < upper {
			slices.push(&list[lower .. upper]);
		}
	}

	OutputIter {lists: slices }
}
```

This is what you might imagine, except that if you are familiar with Rust you might be surprised that `binary_search` is used as a free method, rather than using `list.binary_search(key)`. Unfortunately, Rust's binary search method doesn't find the *first* matching element, just *a* matching element. To find the actual range we would need to search backwards and forwards, which goes against our "don't scan the elements unless asked". 

Fortunately it is easier to write your own binary search than it is to work around Rust's quirky behavior.

```rust
/// Returns the count of the number of elements satisfying `test`.
///
/// Correct behavior relies on true values for `test` coming before
/// false values in `slice`.
fn binary_search<T, F: Fn(&T)->bool>(slice: &[T], test: F) -> usize {

	// bounds on result.
	let mut lower = 0;
	let mut upper = slice.len();

	while lower < upper {
		let mid = (lower + upper) / 2;
		if test(&slice[mid]) {
			lower = mid + 1;
		}
		else {
			upper = mid;
		}
	}

	assert_eq!(lower, upper);
	lower
}
```

All we need to do now is sort out the `OutputIter` type and we should be set!

#### Iterating outputs

Our `OutputIter` will have a list of sorted slices, and walk through each of them in no particular order. We will improve the order later on. 

```rust
pub struct OutputIter<'a, K:'a, V:'a, T:'a> {
	lists: Vec<&'a [(K,V,T,isize)]>
}
```

Now we need to write an iterator implementation. Since we have no particular order required, we should just look at `lists` and see if it is empty, and if not go and read some element from one of the slices. We then need to advance the slice that we read and clean up any empty slices.

```rust
impl<'a, K:'a, V:'a, T:'a> Iterator for OutputIter<'a, K, V, T> {
	type Item = (&'a K, &'a V, &'a T, isize);
	fn next(&mut self) -> Option<Self::Item> {
		if let Some(mut last) = self.lists.pop() {
			let result = (&last[0].0, &last[0].1, &last[0].2, last[0].3);
			last = &last[1..];
			if last.len() > 0 {
				self.lists.push(last);
			}
			Some(result)
		}
		else {
			None
		}
	}
}
```

This implementation repeatedly pops and pushes things from `self.lists`, which feels a bit silly and I'm hoping that LLVM will optimize that away. Otherwise it is a bit of a pain to get a mutable reference to the last list (so it can be advanced), and then mutate the list holding on to it. You can, but you have to flatten out the control flow a bit and pass some `bool`s around.

### Looking at performance

Despite having a very simple implementation, we might still be interested in how this implementation behaves. In particular, it would be great to know which of the highlighted issues are the main pain points, and maybe which pain points haven't yet been highlighted.

#### Insertion throughput

Let's see how quickly we can put elements into our trace. Insertion throughput is one of the reasons we are avoiding a `HashMap` implementation, so we would like this to be reasonably good. We'll start by just trying to understand it.

I've written a `profile-lists.rs` program that takes a number of keys, number of values, and batch size, and repeatedly introduces batches of records whose keys and values are picked by an increasing counter mod the number of valid possibilities. It prints out after every so-many batches.

##### A hash map baseline

Before we get started, I ran the computation with a `HashMap<K, Vec<(V,T,isize)>` as written up earlier. It doesn't implement the `Trace` trait, but we can still run it to get a feel. Here is how it looks:

	Echidnatron% cargo run --release --bin profile-lists -- 1000 1000 1000 1000
	     Running `target/release/profile-lists 1000 1000 1000 1000`
	throughput: 16517799.18 elts/sec @ 1000000 elts
	throughput: 16582305.02 elts/sec @ 2000000 elts
	throughput: 15579539.55 elts/sec @ 3000000 elts
	throughput: 16189958.99 elts/sec @ 4000000 elts
	throughput: 14807005.80 elts/sec @ 5000000 elts
	...
	throughput: 8571400.89 elts/sec @ 99000000 elts
	throughput: 8587396.41 elts/sec @ 100000000 elts
	throughput: 8603516.77 elts/sec @ 101000000 elts
	...
	throughput: 8138150.61 elts/sec @ 522000000 elts
	throughput: 8137500.97 elts/sec @ 523000000 elts
	throughput: 8136678.71 elts/sec @ 524000000 elts

At this point the process crossed the 16GB threshold and I stopped the experiment. But, we have a starting baseline for insertion throughput: between 8 and 16 million elements per second. You could probably do better, for example [here is a fun read](https://infosys.uni-saarland.de/publications/ARCD15.pdf) where they are worried about a lot more keys than I used (them: 16M, 256M, 1B; here: 1K). 

Just to set the scale, in case you would like to compare to their Figures 2 and 3, which report 20 million inserts per second, here are some `HashMap` numbers with 16M keys:

	Echidnatron% cargo run --release --bin profile-lists -- 16000000 1000 1000 1000
	     Running `target/release/profile-lists 16000000 1000 1000 1000`
	throughput: 2578612.44 elts/sec @ 1000000 elts
	throughput: 2446594.26 elts/sec @ 2000000 elts
	throughput: 2782877.84 elts/sec @ 3000000 elts
	throughput: 2339393.02 elts/sec @ 4000000 elts
	throughput: 2522213.50 elts/sec @ 5000000 elts
	...
	throughput: 3296886.79 elts/sec @ 99000000 elts
	throughput: 3304827.24 elts/sec @ 100000000 elts
	throughput: 3312735.90 elts/sec @ 101000000 elts

These numbers start a bit shaky and improve, probably because for the first 16 million elements the hash map is still getting properly sized. But substantially less than the paper above. If we turn things up to 256 million keys, the hash map does this:

	Echidnatron% cargo run --release --bin profile-lists -- 256000000 1000 1000 1000
	     Running `target/release/profile-lists 256000000 1000 1000 1000`
	throughput: 2529228.46 elts/sec @ 1000000 elts
	throughput: 2496031.20 elts/sec @ 2000000 elts
	throughput: 2821726.46 elts/sec @ 3000000 elts
	throughput: 2392483.92 elts/sec @ 4000000 elts
	throughput: 2577791.26 elts/sec @ 5000000 elts
	...
	throughput: 1867181.91 elts/sec @ 120000000 elts
	throughput: 1855464.96 elts/sec @ 121000000 elts
	throughput: 1839465.73 elts/sec @ 122000000 elts

This falls over quite a bit earlier, because the hash map has a fair amount of per-key state. We could try one billion keys, but you'll notice that we only managed 122 million insertions above, so we didn't even get the opportunity to repeat a key. The same thing should happen with one billion keys.

So, depending on the numbers of keys we see a range of throughputs with the very simple `HashMap` implementation: from just shy of two million inserts per second, up to 16 million inserts per second. The [linked paper](https://infosys.uni-saarland.de/publications/ARCD15.pdf) shows a pretty consistent 20 million single-threaded inserts per second, even with large numbers of keys, through engineering and possibly some different assumptions. 

##### Our most na√Øve implementation

Let's slot in our sorted list implementation. Get ready for some worse numbers!

For 1000 keys and 1000 values, batch size 1000, printing every 1000, the numbers look like:

	Echidnatron% cargo run --release --bin profile-lists -- 1000 1000 1000 1000
	     Running `target/release/profile-lists 1000 1000 1000 1000`
	throughput: 2419446.83 elts/sec @ 1000000 elts
	throughput: 2157777.54 elts/sec @ 2000000 elts
	throughput: 1907561.41 elts/sec @ 3000000 elts
	throughput: 1939368.69 elts/sec @ 4000000 elts
	throughput: 1694788.02 elts/sec @ 5000000 elts
	...
	throughput: 1204330.69 elts/sec @ 99000000 elts
	throughput: 1210728.25 elts/sec @ 100000000 elts
	throughput: 1215091.39 elts/sec @ 101000000 elts
	...
	throughput: 1092312.51 elts/sec @ 260000000 elts
	throughput: 1094232.04 elts/sec @ 261000000 elts
	throughput: 1096708.09 elts/sec @ 262000000 elts

You can see where this is going. Actually, nowhere because it ran out of memory at this point, but the throughput goes down as we run: we are only appending data and never removing anything. Given that our per-record work is "logarithmic", if we keep running forever eventually that is going to increase. The good news is we will run out of memory before it becomes too serious.

These numbers aren't so great. They are about one-third the number for the many-keys `HashMap`, and one-tenth the numbers for the thousand-keys `HashMap`. On the positive side, these numbers don't vary at all as we change the numbers of keys, though being consistently slow isn't much of a technical achievement.

One of the reasons the code is slow is that we are re-sorting appended sorted data, rather than merging it. Let's say we try to do merging correctly. I've written the following merging code:

```rust
/// Merges two sorted vectors into a new result vector
fn merge<T: Ord>(source1: Vec<T>, source2: Vec<T>) -> Vec<T> {

	// pre-allocate the required amount of space.
	let mut result = Vec::with_capacity(source1.len() + source2.len());

	// convert vectors into peekable iterators.
	let mut iter1 = source1.into_iter().peekable();
	let mut iter2 = source2.into_iter().peekable();

	// while comparisons remain, do them and move elements.
	while iter1.peek().is_some() && iter2.peek().is_some() {
		match iter1.peek().unwrap().cmp(&iter2.peek().unwrap()) {
			::std::cmp::Ordering::Less => { 
				result.push(iter1.next().unwrap()); 
			},
			::std::cmp::Ordering::Equal => { 
				result.push(iter1.next().unwrap()); 
				result.push(iter2.next().unwrap()); 
			}
			::std::cmp::Ordering::Greater => { 
				result.push(iter2.next().unwrap()); 
			}		
		}
	}

	// drain whatever remains.
	result.extend(iter1);
	result.extend(iter2);

	result
}
```

If we replace the `buffer.extend(..)` line with `buffer = merge(buffer, ...)` and move the `buffer.sort()` from the end to just after it is collected, we get some better numbers:

	Echidnatron% cargo run --release --bin profile-lists -- 1000 1000 1000 1000
	     Running `target/release/profile-lists 1000 1000 1000 1000`
	throughput: 6339778.39 elts/sec @ 1000000 elts
	throughput: 5848068.07 elts/sec @ 2000000 elts
	throughput: 5465519.50 elts/sec @ 3000000 elts
	throughput: 5684741.96 elts/sec @ 4000000 elts
	throughput: 5109088.94 elts/sec @ 5000000 elts
	...
	throughput: 4222326.35 elts/sec @ 99000000 elts
	throughput: 4243203.70 elts/sec @ 100000000 elts
	throughput: 4258128.61 elts/sec @ 101000000 elts
	...
	throughput: 3949494.99 elts/sec @ 522000000 elts
	throughput: 3953483.23 elts/sec @ 523000000 elts
	throughput: 3957824.17 elts/sec @ 524000000 elts

It falls over a little bit later because `merge` allocates half as much memory as `buffer.sort()`. 

We are now doing better than the many-keys `HashMap` numbers, though not as good as the thousand-keys numbers. This feels pretty good, though if we were to consider lookup times the feeling would pass.

So that we can feel even better about ourselves before we close out our discussion of the very na√Øve version, let's test a feature that we will eventually want: consolidation of weights for elements with the same `(key,val,time)` fields. As part of `merge`, we can compare just the `(K, V, T)` part of each record, and add weights if they are the same rather than returning both elements. 

	Echidnatron% cargo run --release --bin profile-lists -- 1000 1000 1000 1000
	     Running `target/release/profile-lists 1000 1000 1000 1000`
	throughput: 22862046.57 elts/sec @ 1000000 elts
	throughput: 21750026.25 elts/sec @ 2000000 elts
	throughput: 21832599.59 elts/sec @ 3000000 elts
	throughput: 21723036.50 elts/sec @ 4000000 elts
	throughput: 21753468.62 elts/sec @ 5000000 elts
	...
	throughput: 20459218.12 elts/sec @ 99000000 elts
	throughput: 20448964.09 elts/sec @ 100000000 elts
	throughput: 20447100.05 elts/sec @ 101000000 elts
	...
	throughput: 20240488.38 elts/sec @ 999000000 elts
	throughput: 20242328.79 elts/sec @ 1000000000 elts
	throughput: 20242923.04 elts/sec @ 1001000000 elts
	...
	throughput: 20447534.22 elts/sec @ 9999000000 elts
	throughput: 20447501.86 elts/sec @ 10000000000 elts
	throughput: 20447385.10 elts/sec @ 10001000000 elts

Ok, now we are talking. Twenty million inserts per second. This should actually still degrade, just much more slowly; the process had reach about 304MB at this point, ten billion elements in.

What happened? The way the code is written the `time` field only gets bumped after each million elements, and because the data are a bit degenerate, with exactly as many keys as values, there are only one thousand distinct values in that million elements. They get compacted quite well! Each round of one million inserts is compacted down to one thousand, before being merged with any older elements.

This is perhaps an interesting representation of what a hot set of updates might look like, with periodic transitions to new hot elements. Perhaps not. 

We can do a different experiment where we have 16 million keys, one value, and one timestamp, something like the 16 million key example above where we are able to update in place.

	Echidnatron% cargo run --release --bin profile-lists -- 16000000 1 1000 1000
	     Running `target/release/profile-lists 16000000 1 1000 1000`
	throughput: 6695085.83 elts/sec @ 1000000 elts
	throughput: 6019800.15 elts/sec @ 2000000 elts
	throughput: 5659953.08 elts/sec @ 3000000 elts
	throughput: 5808231.16 elts/sec @ 4000000 elts
	throughput: 5213819.41 elts/sec @ 5000000 elts
	...
	throughput: 4146630.65 elts/sec @ 99000000 elts
	throughput: 4167738.99 elts/sec @ 100000000 elts
	throughput: 4176494.03 elts/sec @ 101000000 elts
	...
	throughput: 4063433.45 elts/sec @ 999000000 elts
	throughput: 4053770.59 elts/sec @ 1000000000 elts
	throughput: 4055836.85 elts/sec @ 1001000000 elts
	...

This looks like it is slowing down a bit, but it shouldn't really be (dunno!). It is sitting steady around between 600 megabytes and 1GB (up and down a bit, as lists come and go). For 16 million 32 byte records (multiplied: 512MB) this lines up with our expectations perfectly. This is better than our hash table implementation up above, and with a much more compact memory footprint. It is yet not better than the informative linked paper, and may not become so, but I think it is a good start; we have more plans in store!

#### Look-up throughput

Looking things up is going to be slow. We are going to improve it later on (hint: "index"), but for now it is slow. It is dinner time, so I'm not going to measure things for y'all, but binary search is much slower than looking something up in a hash map.

## Part 3: Organizing the data into tries.

Coming later!

## Part 4: Adding an index.

Coming even later!